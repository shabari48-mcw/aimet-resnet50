{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from data_pipeline import DataPipeline\n",
    "\n",
    "DATA_DIR = '/media/bmw/datasets/imagenet-1k/val'\n",
    "MODEL_DIR = 'resnet50.onnx'\n",
    "\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torchvision.models as models\n",
    "\n",
    "resnet50 = models.resnet50()\n",
    "resnet50.load_state_dict(torch.load(\"resnet50.pth\",weights_only=True))\n",
    "resnet50.eval()\n",
    "resnet50.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Model according to Model Guidelines using Model Preparator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:48:56,214 - root - INFO - AIMET\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmw/anaconda3/envs/shabari/lib/python3.12/site-packages/onnxscript/converter.py:823: FutureWarning: 'onnxscript.values.Op.param_schemas' is deprecated in version 0.1 and will be removed in the future. Please use '.op_signature' instead.\n",
      "  param_schemas = callee.param_schemas()\n",
      "<frozen abc>:106: FutureWarning: `NLLLoss2d` has been deprecated. Please use `NLLLoss` instead as a drop-in replacement and see https://pytorch.org/docs/main/nn.html#torch.nn.NLLLoss for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:48:57,316 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer1.0.module_relu_1} \n",
      "2025-02-19 16:48:57,317 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer1.0.module_add} \n",
      "2025-02-19 16:48:57,317 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer1.0.module_relu_2} \n",
      "2025-02-19 16:48:57,317 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer1.1.module_relu_1} \n",
      "2025-02-19 16:48:57,318 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer1.1.module_add_1} \n",
      "2025-02-19 16:48:57,318 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer1.1.module_relu_2} \n",
      "2025-02-19 16:48:57,318 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer1.2.module_relu_1} \n",
      "2025-02-19 16:48:57,319 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer1.2.module_add_2} \n",
      "2025-02-19 16:48:57,319 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer1.2.module_relu_2} \n",
      "2025-02-19 16:48:57,319 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.0.module_relu_1} \n",
      "2025-02-19 16:48:57,320 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer2.0.module_add_3} \n",
      "2025-02-19 16:48:57,320 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.0.module_relu_2} \n",
      "2025-02-19 16:48:57,320 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.1.module_relu_1} \n",
      "2025-02-19 16:48:57,321 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer2.1.module_add_4} \n",
      "2025-02-19 16:48:57,321 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.1.module_relu_2} \n",
      "2025-02-19 16:48:57,321 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.2.module_relu_1} \n",
      "2025-02-19 16:48:57,322 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer2.2.module_add_5} \n",
      "2025-02-19 16:48:57,322 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.2.module_relu_2} \n",
      "2025-02-19 16:48:57,322 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.3.module_relu_1} \n",
      "2025-02-19 16:48:57,323 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer2.3.module_add_6} \n",
      "2025-02-19 16:48:57,323 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer2.3.module_relu_2} \n",
      "2025-02-19 16:48:57,324 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.0.module_relu_1} \n",
      "2025-02-19 16:48:57,324 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer3.0.module_add_7} \n",
      "2025-02-19 16:48:57,325 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.0.module_relu_2} \n",
      "2025-02-19 16:48:57,325 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.1.module_relu_1} \n",
      "2025-02-19 16:48:57,325 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer3.1.module_add_8} \n",
      "2025-02-19 16:48:57,326 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.1.module_relu_2} \n",
      "2025-02-19 16:48:57,326 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.2.module_relu_1} \n",
      "2025-02-19 16:48:57,326 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer3.2.module_add_9} \n",
      "2025-02-19 16:48:57,327 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.2.module_relu_2} \n",
      "2025-02-19 16:48:57,327 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.3.module_relu_1} \n",
      "2025-02-19 16:48:57,327 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer3.3.module_add_10} \n",
      "2025-02-19 16:48:57,328 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.3.module_relu_2} \n",
      "2025-02-19 16:48:57,328 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.4.module_relu_1} \n",
      "2025-02-19 16:48:57,328 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer3.4.module_add_11} \n",
      "2025-02-19 16:48:57,329 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.4.module_relu_2} \n",
      "2025-02-19 16:48:57,329 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.5.module_relu_1} \n",
      "2025-02-19 16:48:57,329 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer3.5.module_add_12} \n",
      "2025-02-19 16:48:57,329 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer3.5.module_relu_2} \n",
      "2025-02-19 16:48:57,330 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer4.0.module_relu_1} \n",
      "2025-02-19 16:48:57,330 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer4.0.module_add_13} \n",
      "2025-02-19 16:48:57,331 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer4.0.module_relu_2} \n",
      "2025-02-19 16:48:57,331 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer4.1.module_relu_1} \n",
      "2025-02-19 16:48:57,331 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer4.1.module_add_14} \n",
      "2025-02-19 16:48:57,331 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer4.1.module_relu_2} \n",
      "2025-02-19 16:48:57,332 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer4.2.module_relu_1} \n",
      "2025-02-19 16:48:57,332 - ModelPreparer - INFO - Functional         : Adding new module for node: {layer4.2.module_add_15} \n",
      "2025-02-19 16:48:57,332 - ModelPreparer - INFO - Reused/Duplicate   : Adding new module for node: {layer4.2.module_relu_2} \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from aimet_torch.model_preparer import prepare_model    \n",
    "    \n",
    "model = prepare_model(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = False\n",
    "if torch.cuda.is_available():\n",
    "    use_cuda = True\n",
    "    model.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the Model using ModelValidator API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:49:21,029 - Utils - INFO - Running validator check <function validate_for_reused_modules at 0x7f1eae4ca7a0>\n",
      "2025-02-19 16:49:21,354 - Utils - INFO - Running validator check <function validate_for_missing_modules at 0x7f1eae0ebec0>\n",
      "2025-02-19 16:49:21,759 - Utils - INFO - All validation checks passed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from aimet_torch.model_validator.model_validator import ModelValidator\n",
    "\n",
    "input_tensor=torch.randn(1,3,224,224)\n",
    "ModelValidator.validate_model(model, model_input=input_tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "input_tensor=input_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP32-Model Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 4/4 [00:01<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 87.00%\n",
      "Top-5 Accuracy: 97.00%\n",
      "Total Samples Evaluated: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DataPipeline.evaluate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform Batch Normalization Folding before quant simulation as it\n",
    "#### improves inference performance on quantized runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aimet_torch.batch_norm_fold import fold_all_batch_norms\n",
    "\n",
    "_ = fold_all_batch_norms(model, input_shapes=(1, 3, 224, 224),dummy_input=input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization Simulation in Batch Normalized Model  W8A8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-19 16:50:48,960 - ConnectedGraph - WARNING - Unable to isolate model outputs.\n",
      "2025-02-19 16:50:49,008 - Quant - INFO - No config file provided, defaulting to config file at /home/bmw/anaconda3/envs/shabari/lib/python3.12/site-packages/aimet_common/quantsim_config/default_config.json\n",
      "2025-02-19 16:50:49,030 - Quant - INFO - Unsupported op type Squeeze\n",
      "2025-02-19 16:50:49,030 - Quant - INFO - Unsupported op type Mean\n",
      "2025-02-19 16:50:49,037 - Quant - INFO - Selecting DefaultOpInstanceConfigGenerator to compute the specialized config. hw_version:default\n"
     ]
    }
   ],
   "source": [
    "from aimet_torch.quantsim import QuantizationSimModel\n",
    "from aimet_common.defs import QuantScheme\n",
    "\n",
    "sim=QuantizationSimModel(model, dummy_input=input_tensor,quant_scheme=QuantScheme.training_range_learning_with_tf_init,  default_output_bw=8,\n",
    "                           default_param_bw=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (conv1): QuantizedConv2d(\n",
      "    3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3)\n",
      "    (param_quantizers): ModuleDict(\n",
      "      (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "      (bias): None\n",
      "    )\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "  )\n",
      "  (bn1): Identity()\n",
      "  (relu): QuantizedReLU(\n",
      "    inplace=True\n",
      "    (param_quantizers): ModuleDict()\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (maxpool): QuantizedMaxPool2d(\n",
      "    kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False\n",
      "    (param_quantizers): ModuleDict()\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (layer1): Module(\n",
      "    (0): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        64, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (downsample): Module(\n",
      "        (0): QuantizedConv2d(\n",
      "          64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_1): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        256, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        64, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_2): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Module(\n",
      "    (0): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        256, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        128, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (downsample): Module(\n",
      "        (0): QuantizedConv2d(\n",
      "          256, 512, kernel_size=(1, 1), stride=(2, 2)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_3): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        128, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_4): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        128, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_5): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        512, 128, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        128, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_6): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Module(\n",
      "    (0): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        512, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (downsample): Module(\n",
      "        (0): QuantizedConv2d(\n",
      "          512, 1024, kernel_size=(1, 1), stride=(2, 2)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_7): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_8): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_9): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_10): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_11): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (5): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        1024, 256, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        256, 1024, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_12): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Module(\n",
      "    (0): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        1024, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        512, 2048, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (downsample): Module(\n",
      "        (0): QuantizedConv2d(\n",
      "          1024, 2048, kernel_size=(1, 1), stride=(2, 2)\n",
      "          (param_quantizers): ModuleDict(\n",
      "            (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "            (bias): None\n",
      "          )\n",
      "          (input_quantizers): ModuleList(\n",
      "            (0): None\n",
      "          )\n",
      "          (output_quantizers): ModuleList(\n",
      "            (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "          )\n",
      "        )\n",
      "        (1): Identity()\n",
      "      )\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_13): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        2048, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        512, 2048, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_14): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): Module(\n",
      "      (conv1): QuantizedConv2d(\n",
      "        2048, 512, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn1): Identity()\n",
      "      (relu): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (conv2): QuantizedConv2d(\n",
      "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (bn2): Identity()\n",
      "      (conv3): QuantizedConv2d(\n",
      "        512, 2048, kernel_size=(1, 1), stride=(1, 1)\n",
      "        (param_quantizers): ModuleDict(\n",
      "          (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "          (bias): None\n",
      "        )\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (bn3): Identity()\n",
      "      (module_relu_1): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "      (module_add_15): QuantizedAdd(\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0-1): 2 x None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "      )\n",
      "      (module_relu_2): QuantizedReLU(\n",
      "        inplace=True\n",
      "        (param_quantizers): ModuleDict()\n",
      "        (input_quantizers): ModuleList(\n",
      "          (0): None\n",
      "        )\n",
      "        (output_quantizers): ModuleList(\n",
      "          (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (avgpool): QuantizedAdaptiveAvgPool2d(\n",
      "    output_size=(1, 1)\n",
      "    (param_quantizers): ModuleDict()\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): QuantizedLinear(\n",
      "    in_features=2048, out_features=1000, bias=True\n",
      "    (param_quantizers): ModuleDict(\n",
      "      (weight): QuantizeDequantize(shape=(), qmin=-128, qmax=127, symmetric=True)\n",
      "      (bias): None\n",
      "    )\n",
      "    (input_quantizers): ModuleList(\n",
      "      (0): None\n",
      "    )\n",
      "    (output_quantizers): ModuleList(\n",
      "      (0): QuantizeDequantize(shape=(), qmin=0, qmax=255, symmetric=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
      "    conv1 = self.conv1(x);  x = None\n",
      "    bn1 = self.bn1(conv1);  conv1 = None\n",
      "    relu = self.relu(bn1);  bn1 = None\n",
      "    maxpool = self.maxpool(relu);  relu = None\n",
      "    layer1_0_conv1 = getattr(self.layer1, \"0\").conv1(maxpool)\n",
      "    layer1_0_bn1 = getattr(self.layer1, \"0\").bn1(layer1_0_conv1);  layer1_0_conv1 = None\n",
      "    layer1_0_relu = getattr(self.layer1, \"0\").relu(layer1_0_bn1);  layer1_0_bn1 = None\n",
      "    layer1_0_conv2 = getattr(self.layer1, \"0\").conv2(layer1_0_relu);  layer1_0_relu = None\n",
      "    layer1_0_bn2 = getattr(self.layer1, \"0\").bn2(layer1_0_conv2);  layer1_0_conv2 = None\n",
      "    layer1_0_module_relu_1 = getattr(self.layer1, \"0\").module_relu_1(layer1_0_bn2);  layer1_0_bn2 = None\n",
      "    layer1_0_conv3 = getattr(self.layer1, \"0\").conv3(layer1_0_module_relu_1);  layer1_0_module_relu_1 = None\n",
      "    layer1_0_bn3 = getattr(self.layer1, \"0\").bn3(layer1_0_conv3);  layer1_0_conv3 = None\n",
      "    layer1_0_downsample_0 = getattr(getattr(self.layer1, \"0\").downsample, \"0\")(maxpool);  maxpool = None\n",
      "    layer1_0_downsample_1 = getattr(getattr(self.layer1, \"0\").downsample, \"1\")(layer1_0_downsample_0);  layer1_0_downsample_0 = None\n",
      "    layer1_0_module_add = getattr(self.layer1, \"0\").module_add(layer1_0_bn3, layer1_0_downsample_1);  layer1_0_bn3 = layer1_0_downsample_1 = None\n",
      "    layer1_0_module_relu_2 = getattr(self.layer1, \"0\").module_relu_2(layer1_0_module_add);  layer1_0_module_add = None\n",
      "    layer1_1_conv1 = getattr(self.layer1, \"1\").conv1(layer1_0_module_relu_2)\n",
      "    layer1_1_bn1 = getattr(self.layer1, \"1\").bn1(layer1_1_conv1);  layer1_1_conv1 = None\n",
      "    layer1_1_relu = getattr(self.layer1, \"1\").relu(layer1_1_bn1);  layer1_1_bn1 = None\n",
      "    layer1_1_conv2 = getattr(self.layer1, \"1\").conv2(layer1_1_relu);  layer1_1_relu = None\n",
      "    layer1_1_bn2 = getattr(self.layer1, \"1\").bn2(layer1_1_conv2);  layer1_1_conv2 = None\n",
      "    layer1_1_module_relu_1 = getattr(self.layer1, \"1\").module_relu_1(layer1_1_bn2);  layer1_1_bn2 = None\n",
      "    layer1_1_conv3 = getattr(self.layer1, \"1\").conv3(layer1_1_module_relu_1);  layer1_1_module_relu_1 = None\n",
      "    layer1_1_bn3 = getattr(self.layer1, \"1\").bn3(layer1_1_conv3);  layer1_1_conv3 = None\n",
      "    layer1_1_module_add_1 = getattr(self.layer1, \"1\").module_add_1(layer1_1_bn3, layer1_0_module_relu_2);  layer1_1_bn3 = layer1_0_module_relu_2 = None\n",
      "    layer1_1_module_relu_2 = getattr(self.layer1, \"1\").module_relu_2(layer1_1_module_add_1);  layer1_1_module_add_1 = None\n",
      "    layer1_2_conv1 = getattr(self.layer1, \"2\").conv1(layer1_1_module_relu_2)\n",
      "    layer1_2_bn1 = getattr(self.layer1, \"2\").bn1(layer1_2_conv1);  layer1_2_conv1 = None\n",
      "    layer1_2_relu = getattr(self.layer1, \"2\").relu(layer1_2_bn1);  layer1_2_bn1 = None\n",
      "    layer1_2_conv2 = getattr(self.layer1, \"2\").conv2(layer1_2_relu);  layer1_2_relu = None\n",
      "    layer1_2_bn2 = getattr(self.layer1, \"2\").bn2(layer1_2_conv2);  layer1_2_conv2 = None\n",
      "    layer1_2_module_relu_1 = getattr(self.layer1, \"2\").module_relu_1(layer1_2_bn2);  layer1_2_bn2 = None\n",
      "    layer1_2_conv3 = getattr(self.layer1, \"2\").conv3(layer1_2_module_relu_1);  layer1_2_module_relu_1 = None\n",
      "    layer1_2_bn3 = getattr(self.layer1, \"2\").bn3(layer1_2_conv3);  layer1_2_conv3 = None\n",
      "    layer1_2_module_add_2 = getattr(self.layer1, \"2\").module_add_2(layer1_2_bn3, layer1_1_module_relu_2);  layer1_2_bn3 = layer1_1_module_relu_2 = None\n",
      "    layer1_2_module_relu_2 = getattr(self.layer1, \"2\").module_relu_2(layer1_2_module_add_2);  layer1_2_module_add_2 = None\n",
      "    layer2_0_conv1 = getattr(self.layer2, \"0\").conv1(layer1_2_module_relu_2)\n",
      "    layer2_0_bn1 = getattr(self.layer2, \"0\").bn1(layer2_0_conv1);  layer2_0_conv1 = None\n",
      "    layer2_0_relu = getattr(self.layer2, \"0\").relu(layer2_0_bn1);  layer2_0_bn1 = None\n",
      "    layer2_0_conv2 = getattr(self.layer2, \"0\").conv2(layer2_0_relu);  layer2_0_relu = None\n",
      "    layer2_0_bn2 = getattr(self.layer2, \"0\").bn2(layer2_0_conv2);  layer2_0_conv2 = None\n",
      "    layer2_0_module_relu_1 = getattr(self.layer2, \"0\").module_relu_1(layer2_0_bn2);  layer2_0_bn2 = None\n",
      "    layer2_0_conv3 = getattr(self.layer2, \"0\").conv3(layer2_0_module_relu_1);  layer2_0_module_relu_1 = None\n",
      "    layer2_0_bn3 = getattr(self.layer2, \"0\").bn3(layer2_0_conv3);  layer2_0_conv3 = None\n",
      "    layer2_0_downsample_0 = getattr(getattr(self.layer2, \"0\").downsample, \"0\")(layer1_2_module_relu_2);  layer1_2_module_relu_2 = None\n",
      "    layer2_0_downsample_1 = getattr(getattr(self.layer2, \"0\").downsample, \"1\")(layer2_0_downsample_0);  layer2_0_downsample_0 = None\n",
      "    layer2_0_module_add_3 = getattr(self.layer2, \"0\").module_add_3(layer2_0_bn3, layer2_0_downsample_1);  layer2_0_bn3 = layer2_0_downsample_1 = None\n",
      "    layer2_0_module_relu_2 = getattr(self.layer2, \"0\").module_relu_2(layer2_0_module_add_3);  layer2_0_module_add_3 = None\n",
      "    layer2_1_conv1 = getattr(self.layer2, \"1\").conv1(layer2_0_module_relu_2)\n",
      "    layer2_1_bn1 = getattr(self.layer2, \"1\").bn1(layer2_1_conv1);  layer2_1_conv1 = None\n",
      "    layer2_1_relu = getattr(self.layer2, \"1\").relu(layer2_1_bn1);  layer2_1_bn1 = None\n",
      "    layer2_1_conv2 = getattr(self.layer2, \"1\").conv2(layer2_1_relu);  layer2_1_relu = None\n",
      "    layer2_1_bn2 = getattr(self.layer2, \"1\").bn2(layer2_1_conv2);  layer2_1_conv2 = None\n",
      "    layer2_1_module_relu_1 = getattr(self.layer2, \"1\").module_relu_1(layer2_1_bn2);  layer2_1_bn2 = None\n",
      "    layer2_1_conv3 = getattr(self.layer2, \"1\").conv3(layer2_1_module_relu_1);  layer2_1_module_relu_1 = None\n",
      "    layer2_1_bn3 = getattr(self.layer2, \"1\").bn3(layer2_1_conv3);  layer2_1_conv3 = None\n",
      "    layer2_1_module_add_4 = getattr(self.layer2, \"1\").module_add_4(layer2_1_bn3, layer2_0_module_relu_2);  layer2_1_bn3 = layer2_0_module_relu_2 = None\n",
      "    layer2_1_module_relu_2 = getattr(self.layer2, \"1\").module_relu_2(layer2_1_module_add_4);  layer2_1_module_add_4 = None\n",
      "    layer2_2_conv1 = getattr(self.layer2, \"2\").conv1(layer2_1_module_relu_2)\n",
      "    layer2_2_bn1 = getattr(self.layer2, \"2\").bn1(layer2_2_conv1);  layer2_2_conv1 = None\n",
      "    layer2_2_relu = getattr(self.layer2, \"2\").relu(layer2_2_bn1);  layer2_2_bn1 = None\n",
      "    layer2_2_conv2 = getattr(self.layer2, \"2\").conv2(layer2_2_relu);  layer2_2_relu = None\n",
      "    layer2_2_bn2 = getattr(self.layer2, \"2\").bn2(layer2_2_conv2);  layer2_2_conv2 = None\n",
      "    layer2_2_module_relu_1 = getattr(self.layer2, \"2\").module_relu_1(layer2_2_bn2);  layer2_2_bn2 = None\n",
      "    layer2_2_conv3 = getattr(self.layer2, \"2\").conv3(layer2_2_module_relu_1);  layer2_2_module_relu_1 = None\n",
      "    layer2_2_bn3 = getattr(self.layer2, \"2\").bn3(layer2_2_conv3);  layer2_2_conv3 = None\n",
      "    layer2_2_module_add_5 = getattr(self.layer2, \"2\").module_add_5(layer2_2_bn3, layer2_1_module_relu_2);  layer2_2_bn3 = layer2_1_module_relu_2 = None\n",
      "    layer2_2_module_relu_2 = getattr(self.layer2, \"2\").module_relu_2(layer2_2_module_add_5);  layer2_2_module_add_5 = None\n",
      "    layer2_3_conv1 = getattr(self.layer2, \"3\").conv1(layer2_2_module_relu_2)\n",
      "    layer2_3_bn1 = getattr(self.layer2, \"3\").bn1(layer2_3_conv1);  layer2_3_conv1 = None\n",
      "    layer2_3_relu = getattr(self.layer2, \"3\").relu(layer2_3_bn1);  layer2_3_bn1 = None\n",
      "    layer2_3_conv2 = getattr(self.layer2, \"3\").conv2(layer2_3_relu);  layer2_3_relu = None\n",
      "    layer2_3_bn2 = getattr(self.layer2, \"3\").bn2(layer2_3_conv2);  layer2_3_conv2 = None\n",
      "    layer2_3_module_relu_1 = getattr(self.layer2, \"3\").module_relu_1(layer2_3_bn2);  layer2_3_bn2 = None\n",
      "    layer2_3_conv3 = getattr(self.layer2, \"3\").conv3(layer2_3_module_relu_1);  layer2_3_module_relu_1 = None\n",
      "    layer2_3_bn3 = getattr(self.layer2, \"3\").bn3(layer2_3_conv3);  layer2_3_conv3 = None\n",
      "    layer2_3_module_add_6 = getattr(self.layer2, \"3\").module_add_6(layer2_3_bn3, layer2_2_module_relu_2);  layer2_3_bn3 = layer2_2_module_relu_2 = None\n",
      "    layer2_3_module_relu_2 = getattr(self.layer2, \"3\").module_relu_2(layer2_3_module_add_6);  layer2_3_module_add_6 = None\n",
      "    layer3_0_conv1 = getattr(self.layer3, \"0\").conv1(layer2_3_module_relu_2)\n",
      "    layer3_0_bn1 = getattr(self.layer3, \"0\").bn1(layer3_0_conv1);  layer3_0_conv1 = None\n",
      "    layer3_0_relu = getattr(self.layer3, \"0\").relu(layer3_0_bn1);  layer3_0_bn1 = None\n",
      "    layer3_0_conv2 = getattr(self.layer3, \"0\").conv2(layer3_0_relu);  layer3_0_relu = None\n",
      "    layer3_0_bn2 = getattr(self.layer3, \"0\").bn2(layer3_0_conv2);  layer3_0_conv2 = None\n",
      "    layer3_0_module_relu_1 = getattr(self.layer3, \"0\").module_relu_1(layer3_0_bn2);  layer3_0_bn2 = None\n",
      "    layer3_0_conv3 = getattr(self.layer3, \"0\").conv3(layer3_0_module_relu_1);  layer3_0_module_relu_1 = None\n",
      "    layer3_0_bn3 = getattr(self.layer3, \"0\").bn3(layer3_0_conv3);  layer3_0_conv3 = None\n",
      "    layer3_0_downsample_0 = getattr(getattr(self.layer3, \"0\").downsample, \"0\")(layer2_3_module_relu_2);  layer2_3_module_relu_2 = None\n",
      "    layer3_0_downsample_1 = getattr(getattr(self.layer3, \"0\").downsample, \"1\")(layer3_0_downsample_0);  layer3_0_downsample_0 = None\n",
      "    layer3_0_module_add_7 = getattr(self.layer3, \"0\").module_add_7(layer3_0_bn3, layer3_0_downsample_1);  layer3_0_bn3 = layer3_0_downsample_1 = None\n",
      "    layer3_0_module_relu_2 = getattr(self.layer3, \"0\").module_relu_2(layer3_0_module_add_7);  layer3_0_module_add_7 = None\n",
      "    layer3_1_conv1 = getattr(self.layer3, \"1\").conv1(layer3_0_module_relu_2)\n",
      "    layer3_1_bn1 = getattr(self.layer3, \"1\").bn1(layer3_1_conv1);  layer3_1_conv1 = None\n",
      "    layer3_1_relu = getattr(self.layer3, \"1\").relu(layer3_1_bn1);  layer3_1_bn1 = None\n",
      "    layer3_1_conv2 = getattr(self.layer3, \"1\").conv2(layer3_1_relu);  layer3_1_relu = None\n",
      "    layer3_1_bn2 = getattr(self.layer3, \"1\").bn2(layer3_1_conv2);  layer3_1_conv2 = None\n",
      "    layer3_1_module_relu_1 = getattr(self.layer3, \"1\").module_relu_1(layer3_1_bn2);  layer3_1_bn2 = None\n",
      "    layer3_1_conv3 = getattr(self.layer3, \"1\").conv3(layer3_1_module_relu_1);  layer3_1_module_relu_1 = None\n",
      "    layer3_1_bn3 = getattr(self.layer3, \"1\").bn3(layer3_1_conv3);  layer3_1_conv3 = None\n",
      "    layer3_1_module_add_8 = getattr(self.layer3, \"1\").module_add_8(layer3_1_bn3, layer3_0_module_relu_2);  layer3_1_bn3 = layer3_0_module_relu_2 = None\n",
      "    layer3_1_module_relu_2 = getattr(self.layer3, \"1\").module_relu_2(layer3_1_module_add_8);  layer3_1_module_add_8 = None\n",
      "    layer3_2_conv1 = getattr(self.layer3, \"2\").conv1(layer3_1_module_relu_2)\n",
      "    layer3_2_bn1 = getattr(self.layer3, \"2\").bn1(layer3_2_conv1);  layer3_2_conv1 = None\n",
      "    layer3_2_relu = getattr(self.layer3, \"2\").relu(layer3_2_bn1);  layer3_2_bn1 = None\n",
      "    layer3_2_conv2 = getattr(self.layer3, \"2\").conv2(layer3_2_relu);  layer3_2_relu = None\n",
      "    layer3_2_bn2 = getattr(self.layer3, \"2\").bn2(layer3_2_conv2);  layer3_2_conv2 = None\n",
      "    layer3_2_module_relu_1 = getattr(self.layer3, \"2\").module_relu_1(layer3_2_bn2);  layer3_2_bn2 = None\n",
      "    layer3_2_conv3 = getattr(self.layer3, \"2\").conv3(layer3_2_module_relu_1);  layer3_2_module_relu_1 = None\n",
      "    layer3_2_bn3 = getattr(self.layer3, \"2\").bn3(layer3_2_conv3);  layer3_2_conv3 = None\n",
      "    layer3_2_module_add_9 = getattr(self.layer3, \"2\").module_add_9(layer3_2_bn3, layer3_1_module_relu_2);  layer3_2_bn3 = layer3_1_module_relu_2 = None\n",
      "    layer3_2_module_relu_2 = getattr(self.layer3, \"2\").module_relu_2(layer3_2_module_add_9);  layer3_2_module_add_9 = None\n",
      "    layer3_3_conv1 = getattr(self.layer3, \"3\").conv1(layer3_2_module_relu_2)\n",
      "    layer3_3_bn1 = getattr(self.layer3, \"3\").bn1(layer3_3_conv1);  layer3_3_conv1 = None\n",
      "    layer3_3_relu = getattr(self.layer3, \"3\").relu(layer3_3_bn1);  layer3_3_bn1 = None\n",
      "    layer3_3_conv2 = getattr(self.layer3, \"3\").conv2(layer3_3_relu);  layer3_3_relu = None\n",
      "    layer3_3_bn2 = getattr(self.layer3, \"3\").bn2(layer3_3_conv2);  layer3_3_conv2 = None\n",
      "    layer3_3_module_relu_1 = getattr(self.layer3, \"3\").module_relu_1(layer3_3_bn2);  layer3_3_bn2 = None\n",
      "    layer3_3_conv3 = getattr(self.layer3, \"3\").conv3(layer3_3_module_relu_1);  layer3_3_module_relu_1 = None\n",
      "    layer3_3_bn3 = getattr(self.layer3, \"3\").bn3(layer3_3_conv3);  layer3_3_conv3 = None\n",
      "    layer3_3_module_add_10 = getattr(self.layer3, \"3\").module_add_10(layer3_3_bn3, layer3_2_module_relu_2);  layer3_3_bn3 = layer3_2_module_relu_2 = None\n",
      "    layer3_3_module_relu_2 = getattr(self.layer3, \"3\").module_relu_2(layer3_3_module_add_10);  layer3_3_module_add_10 = None\n",
      "    layer3_4_conv1 = getattr(self.layer3, \"4\").conv1(layer3_3_module_relu_2)\n",
      "    layer3_4_bn1 = getattr(self.layer3, \"4\").bn1(layer3_4_conv1);  layer3_4_conv1 = None\n",
      "    layer3_4_relu = getattr(self.layer3, \"4\").relu(layer3_4_bn1);  layer3_4_bn1 = None\n",
      "    layer3_4_conv2 = getattr(self.layer3, \"4\").conv2(layer3_4_relu);  layer3_4_relu = None\n",
      "    layer3_4_bn2 = getattr(self.layer3, \"4\").bn2(layer3_4_conv2);  layer3_4_conv2 = None\n",
      "    layer3_4_module_relu_1 = getattr(self.layer3, \"4\").module_relu_1(layer3_4_bn2);  layer3_4_bn2 = None\n",
      "    layer3_4_conv3 = getattr(self.layer3, \"4\").conv3(layer3_4_module_relu_1);  layer3_4_module_relu_1 = None\n",
      "    layer3_4_bn3 = getattr(self.layer3, \"4\").bn3(layer3_4_conv3);  layer3_4_conv3 = None\n",
      "    layer3_4_module_add_11 = getattr(self.layer3, \"4\").module_add_11(layer3_4_bn3, layer3_3_module_relu_2);  layer3_4_bn3 = layer3_3_module_relu_2 = None\n",
      "    layer3_4_module_relu_2 = getattr(self.layer3, \"4\").module_relu_2(layer3_4_module_add_11);  layer3_4_module_add_11 = None\n",
      "    layer3_5_conv1 = getattr(self.layer3, \"5\").conv1(layer3_4_module_relu_2)\n",
      "    layer3_5_bn1 = getattr(self.layer3, \"5\").bn1(layer3_5_conv1);  layer3_5_conv1 = None\n",
      "    layer3_5_relu = getattr(self.layer3, \"5\").relu(layer3_5_bn1);  layer3_5_bn1 = None\n",
      "    layer3_5_conv2 = getattr(self.layer3, \"5\").conv2(layer3_5_relu);  layer3_5_relu = None\n",
      "    layer3_5_bn2 = getattr(self.layer3, \"5\").bn2(layer3_5_conv2);  layer3_5_conv2 = None\n",
      "    layer3_5_module_relu_1 = getattr(self.layer3, \"5\").module_relu_1(layer3_5_bn2);  layer3_5_bn2 = None\n",
      "    layer3_5_conv3 = getattr(self.layer3, \"5\").conv3(layer3_5_module_relu_1);  layer3_5_module_relu_1 = None\n",
      "    layer3_5_bn3 = getattr(self.layer3, \"5\").bn3(layer3_5_conv3);  layer3_5_conv3 = None\n",
      "    layer3_5_module_add_12 = getattr(self.layer3, \"5\").module_add_12(layer3_5_bn3, layer3_4_module_relu_2);  layer3_5_bn3 = layer3_4_module_relu_2 = None\n",
      "    layer3_5_module_relu_2 = getattr(self.layer3, \"5\").module_relu_2(layer3_5_module_add_12);  layer3_5_module_add_12 = None\n",
      "    layer4_0_conv1 = getattr(self.layer4, \"0\").conv1(layer3_5_module_relu_2)\n",
      "    layer4_0_bn1 = getattr(self.layer4, \"0\").bn1(layer4_0_conv1);  layer4_0_conv1 = None\n",
      "    layer4_0_relu = getattr(self.layer4, \"0\").relu(layer4_0_bn1);  layer4_0_bn1 = None\n",
      "    layer4_0_conv2 = getattr(self.layer4, \"0\").conv2(layer4_0_relu);  layer4_0_relu = None\n",
      "    layer4_0_bn2 = getattr(self.layer4, \"0\").bn2(layer4_0_conv2);  layer4_0_conv2 = None\n",
      "    layer4_0_module_relu_1 = getattr(self.layer4, \"0\").module_relu_1(layer4_0_bn2);  layer4_0_bn2 = None\n",
      "    layer4_0_conv3 = getattr(self.layer4, \"0\").conv3(layer4_0_module_relu_1);  layer4_0_module_relu_1 = None\n",
      "    layer4_0_bn3 = getattr(self.layer4, \"0\").bn3(layer4_0_conv3);  layer4_0_conv3 = None\n",
      "    layer4_0_downsample_0 = getattr(getattr(self.layer4, \"0\").downsample, \"0\")(layer3_5_module_relu_2);  layer3_5_module_relu_2 = None\n",
      "    layer4_0_downsample_1 = getattr(getattr(self.layer4, \"0\").downsample, \"1\")(layer4_0_downsample_0);  layer4_0_downsample_0 = None\n",
      "    layer4_0_module_add_13 = getattr(self.layer4, \"0\").module_add_13(layer4_0_bn3, layer4_0_downsample_1);  layer4_0_bn3 = layer4_0_downsample_1 = None\n",
      "    layer4_0_module_relu_2 = getattr(self.layer4, \"0\").module_relu_2(layer4_0_module_add_13);  layer4_0_module_add_13 = None\n",
      "    layer4_1_conv1 = getattr(self.layer4, \"1\").conv1(layer4_0_module_relu_2)\n",
      "    layer4_1_bn1 = getattr(self.layer4, \"1\").bn1(layer4_1_conv1);  layer4_1_conv1 = None\n",
      "    layer4_1_relu = getattr(self.layer4, \"1\").relu(layer4_1_bn1);  layer4_1_bn1 = None\n",
      "    layer4_1_conv2 = getattr(self.layer4, \"1\").conv2(layer4_1_relu);  layer4_1_relu = None\n",
      "    layer4_1_bn2 = getattr(self.layer4, \"1\").bn2(layer4_1_conv2);  layer4_1_conv2 = None\n",
      "    layer4_1_module_relu_1 = getattr(self.layer4, \"1\").module_relu_1(layer4_1_bn2);  layer4_1_bn2 = None\n",
      "    layer4_1_conv3 = getattr(self.layer4, \"1\").conv3(layer4_1_module_relu_1);  layer4_1_module_relu_1 = None\n",
      "    layer4_1_bn3 = getattr(self.layer4, \"1\").bn3(layer4_1_conv3);  layer4_1_conv3 = None\n",
      "    layer4_1_module_add_14 = getattr(self.layer4, \"1\").module_add_14(layer4_1_bn3, layer4_0_module_relu_2);  layer4_1_bn3 = layer4_0_module_relu_2 = None\n",
      "    layer4_1_module_relu_2 = getattr(self.layer4, \"1\").module_relu_2(layer4_1_module_add_14);  layer4_1_module_add_14 = None\n",
      "    layer4_2_conv1 = getattr(self.layer4, \"2\").conv1(layer4_1_module_relu_2)\n",
      "    layer4_2_bn1 = getattr(self.layer4, \"2\").bn1(layer4_2_conv1);  layer4_2_conv1 = None\n",
      "    layer4_2_relu = getattr(self.layer4, \"2\").relu(layer4_2_bn1);  layer4_2_bn1 = None\n",
      "    layer4_2_conv2 = getattr(self.layer4, \"2\").conv2(layer4_2_relu);  layer4_2_relu = None\n",
      "    layer4_2_bn2 = getattr(self.layer4, \"2\").bn2(layer4_2_conv2);  layer4_2_conv2 = None\n",
      "    layer4_2_module_relu_1 = getattr(self.layer4, \"2\").module_relu_1(layer4_2_bn2);  layer4_2_bn2 = None\n",
      "    layer4_2_conv3 = getattr(self.layer4, \"2\").conv3(layer4_2_module_relu_1);  layer4_2_module_relu_1 = None\n",
      "    layer4_2_bn3 = getattr(self.layer4, \"2\").bn3(layer4_2_conv3);  layer4_2_conv3 = None\n",
      "    layer4_2_module_add_15 = getattr(self.layer4, \"2\").module_add_15(layer4_2_bn3, layer4_1_module_relu_2);  layer4_2_bn3 = layer4_1_module_relu_2 = None\n",
      "    layer4_2_module_relu_2 = getattr(self.layer4, \"2\").module_relu_2(layer4_2_module_add_15);  layer4_2_module_add_15 = None\n",
      "    avgpool = self.avgpool(layer4_2_module_relu_2);  layer4_2_module_relu_2 = None\n",
      "    flatten = torch.flatten(avgpool, 1);  avgpool = None\n",
      "    fc = self.fc(flatten);  flatten = None\n",
      "    return fc\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "print(sim.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass Calibration Data (Unlabelled Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pass_calibration_data(sim_model, use_cuda):\n",
    "    data_loader = DataPipeline.get_val_dataloader()\n",
    "    batch_size = data_loader.batch_size\n",
    "\n",
    "    if use_cuda:\n",
    "        device = torch.device('cuda')\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    sim_model.eval()\n",
    "    samples = 1000\n",
    "\n",
    "    batch_cntr = 0\n",
    "    with torch.no_grad():\n",
    "        for input_data, target_data in data_loader:\n",
    "\n",
    "            inputs_batch = input_data.to(device)\n",
    "            sim_model(inputs_batch)\n",
    "\n",
    "            batch_cntr += 1\n",
    "            if (batch_cntr * batch_size) > samples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim.compute_encodings(forward_pass_callback=pass_calibration_data,\n",
    "                      forward_pass_callback_args=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bmw/anaconda3/envs/shabari/lib/python3.12/site-packages/aimet_torch/v2/quantization/affine/backends/torch_builtins.py:75: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not _is_grid_representable(dtype, qmin, qmax):\n",
      "/home/bmw/anaconda3/envs/shabari/lib/python3.12/site-packages/aimet_torch/v2/quantization/affine/backends/torch_builtins.py:175: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not _is_grid_representable(internal_dtype, qmin, qmax):\n",
      "/home/bmw/anaconda3/envs/shabari/lib/python3.12/site-packages/aimet_torch/v2/utils.py:155: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert new_param_or_buffer.shape == orig_param_or_buffer.shape\n",
      "/home/bmw/anaconda3/envs/shabari/lib/python3.12/site-packages/aimet_torch/v2/quantization/affine/backends/torch_builtins.py:56: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  dtype_repr = torch.tensor(value, dtype=dtype)\n",
      "/home/bmw/anaconda3/envs/shabari/lib/python3.12/site-packages/aimet_torch/v2/quantization/affine/backends/torch_builtins.py:57: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return dtype_repr.isfinite() and dtype_repr.long() == value\n",
      "/home/bmw/anaconda3/envs/shabari/lib/python3.12/site-packages/aimet_torch/v2/quantization/affine/backends/torch_builtins.py:65: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  return _is_value_representable(dtype, qmax) and \\\n",
      "Evaluating: 100%|██████████| 4/4 [00:07<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1 Accuracy: 79.00%\n",
      "Top-5 Accuracy: 94.00%\n",
      "Total Samples Evaluated: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "DataPipeline.evaluate(sim.model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shabari",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
